{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apprentissage profond - TD n°1 \n",
    "________________\n",
    "\n",
    "#### Exo 1 : Apprentissage de FizzBuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Modélisation__ \n",
    "\n",
    "On modélise le problème d'apprentissage commme un problème de classification multi-classe.\n",
    "\n",
    "Les sorties possibles 0 / 1 / 2 / 3 correspondent respectivement :\n",
    "* au nombre lui-même, \n",
    "* à `fizz` pour les multiples de 3, \n",
    "* à `buzz` pour les multiples de 5,\n",
    "* à `fizzbuzz` pour les multiples de 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data encoding : ground truth labels \n",
    "def fizz_buzz_encode(i):\n",
    "    res = 0\n",
    "    if i % 15 == 0: # fizzbuzz\n",
    "        res = 3\n",
    "    elif i % 5 == 0 : # buzz\n",
    "        res = 2\n",
    "    elif i % 3 == 0: # fizz\n",
    "        res =  1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding for 3 is 1, corresponding to fizz.\n",
      "1 0 1\n",
      "2 0 2\n",
      "3 1 fizz\n",
      "4 0 4\n",
      "5 2 buzz\n",
      "6 1 fizz\n",
      "7 0 7\n",
      "8 0 8\n",
      "9 1 fizz\n",
      "10 2 buzz\n",
      "11 0 11\n",
      "12 1 fizz\n",
      "13 0 13\n",
      "14 0 14\n",
      "15 3 fizzbuzz\n",
      "16 0 16\n",
      "17 0 17\n",
      "18 1 fizz\n",
      "19 0 19\n"
     ]
    }
   ],
   "source": [
    "# data display (when we have a prediction 0/1/2/3 and we want to get the human-readable result)\n",
    "def fizz_buzz (i , prediction):\n",
    "    return [str(i), \"fizz\", \"buzz\", \"fizzbuzz\"][prediction]\n",
    "\n",
    "# example\n",
    "i, pred = 3, 1 # right prediction\n",
    "print(f\"Encoding for {i} is {pred}, corresponding to {fizz_buzz(i,pred)}.\")\n",
    "\n",
    "# more examples\n",
    "for i in range(1, 20) : \n",
    "    print(i, fizz_buzz_encode(i), fizz_buzz (i , fizz_buzz_encode(i) ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Comment encoder les données d'entrées ?__\n",
    "\n",
    "On propose d'encoder les entiers sous forme binaire au lieu d'utiliser la base 10.\n",
    "\n",
    "*Astuce : Encodage binaire avec les opérateurs binaires python >> et &*\n",
    "* operateur >> (bitwise right shift operator): décalle l'écriture de binaire de d places (padding avec des zéros à gauche), cf animation sur https://realpython.com/python-bitwise-operators/#right-shift \n",
    "* operateur & (bitmask AND): correspond à un ET bit à bit e.g. 13 & 1 = 1101 & 0001 = 0001\n",
    "\n",
    "Par exemple 13 en binaire s'écrit 1101, en appliquant successivement les étapes suivantes, on retrouve la décomposition en binaire:\n",
    "    * 13 >> 0 & 1 = 1101  & 0001 = 1\n",
    "    * 13 >> 1 & 1 = 0110  & 0001 = 0\n",
    "    * 13 >> 2 & 1 = 0011  & 0001 = 1\n",
    "    * 13 >> 3 & 1 = 0001  & 0001 = 0\n",
    "\n",
    "\n",
    "Autrement dit : \n",
    "\n",
    "> a >> n\n",
    "\n",
    "revient à écrire en binaire\n",
    "\n",
    "> np.floor(a/2**n)\n",
    "\n",
    "et\n",
    "\n",
    "> a & n\n",
    "\n",
    "revient à appliquer l'opération AND sur les écritures binaires de a et n "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 1]\n",
      "[1, 1, 0, 1]\n",
      "1\n",
      "13\n",
      "6\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "number = 13\n",
    "nb_digits = 4\n",
    "decomp = [number >> d & 1 for d in range(nb_digits)]\n",
    "print(decomp)\n",
    "decomp.reverse()\n",
    "print(decomp)\n",
    "\n",
    "print(number & 1)\n",
    "print(number >> 0)\n",
    "print(number >> 1)\n",
    "print(number >> 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "1 [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "2 [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "3 [1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "4 [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "5 [1, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "6 [0, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "7 [1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "8 [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "9 [1, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# represent digits by their binary encoding\n",
    "\n",
    "NUM_DIGITS = 10\n",
    "def binary_encode (i , num_digits=NUM_DIGITS ) :\n",
    "    return [i >> d & 1 for d in range(num_digits)]\n",
    "\n",
    "for i in range(10):\n",
    "    print(i, binary_encode(i)) \n",
    "\n",
    "# NB : on les affiche l'encodage binaire \"à l'envers\" mais ça n'a aucun impact d'un point de vue de la modélisation ou de l'apprentissage "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Préparation des données (train et test)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "start_train, end_train = 101, 1023\n",
    "start_test, end_test = 1, 101\n",
    "\n",
    "# train\n",
    "X_train = torch.FloatTensor ([ binary_encode (i , NUM_DIGITS ) for i in range (start_train, end_train)])\n",
    "Y_train = torch.LongTensor ([fizz_buzz_encode(i) for i in range(start_train, end_train)]).squeeze()\n",
    "\n",
    "# test\n",
    "X_test = torch.FloatTensor ([ binary_encode (i , NUM_DIGITS) for i in range (start_test, end_test) ])\n",
    "print(X_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Modèle__\n",
    "\n",
    "Voici une architecture simple de réseau de neurone à 1 seule couche cachée. Vous pouvez expérimenter avec d'autres architectures e.g. un nombre de neurones différents dans la couche cachée, ou deux couches cachées. Attention aux dimensions des couches !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=10, out_features=100, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=100, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# nombre de neurones dans la couche cachée\n",
    "NUM_HIDDEN = 100\n",
    "\n",
    "# définition du MLP à 1 couche cachée (non linearite ReLU)\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(NUM_DIGITS, NUM_HIDDEN),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(NUM_HIDDEN, 4)\n",
    "    )\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Fonction de coût__ \n",
    "\n",
    "Quelle est la différence entre la CrossEntropyLoss et la NLLLoss ?\n",
    "\n",
    "CE loss : Selon la doc PyTorch \"The input is expected to contain the unnormalized logits for each class, *which do not need to be positive or sum to 1, in general*. \" cf https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss. \n",
    "\n",
    "NLLLoss : signifie \"negative log likelihood loss\", attend des entrées positives, typiquement des logits. \n",
    "\n",
    "Dans notre MLP, on n'a pas appliqué de softmax aux sorties de la dernière couche, donc on prend ici la CrossEntropyLoss et pas la NLLLoss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction de coût\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimiseur --> choix du learning rate \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Entrainement__\n",
    "\n",
    "2 limites dans cette première version du code : \n",
    "* on regarde ce que ça donne sur le test set au fur et à mesure de l'entrainement, ce n'est pas une hypothèse réaliste (ou saine)\n",
    "* il vaudrait mieux partager le train set en un train set et un val set, et suivre l'évolution de l'accuracy sur le val set au lieu du test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 training loss 1.19\n",
      "['1' '2' '3' '4' '5' '6' '7' '8' '9' '10' '11' '12' '13' '14' '15' '16'\n",
      " '17' '18' '19' '20' '21' '22' '23' '24' '25' '26' '27' '28' '29' '30'\n",
      " '31' '32' '33' '34' '35' '36' '37' '38' '39' '40' '41' '42' '43' '44'\n",
      " '45' '46' '47' '48' '49' '50' '51' '52' '53' '54' '55' '56' '57' '58'\n",
      " '59' '60' '61' '62' '63' '64' '65' '66' '67' '68' '69' '70' '71' '72'\n",
      " '73' '74' '75' '76' '77' '78' '79' '80' '81' '82' '83' '84' '85' '86'\n",
      " '87' '88' '89' '90' '91' '92' '93' '94' '95' '96' '97' '98' '99' '100']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100 training loss 1.134\n",
      "epoch 200 training loss 1.121\n",
      "epoch 300 training loss 1.1\n",
      "epoch 400 training loss 1.07\n",
      "epoch 500 training loss 1.032\n",
      "epoch 600 training loss 0.983\n",
      "epoch 700 training loss 0.932\n",
      "epoch 800 training loss 0.872\n",
      "epoch 900 training loss 0.813\n",
      "epoch 1000 training loss 0.736\n",
      "['1' '2' 'fizz' '4' 'buzz' '6' '7' '8' '9' 'buzz' '11' '12' '13' '14' '15'\n",
      " '16' '17' 'fizz' '19' 'buzz' '21' '22' '23' '24' '25' '26' '27' '28' '29'\n",
      " '30' '31' '32' 'buzz' '34' 'buzz' '36' '37' '38' '39' 'buzz' '41' 'fizz'\n",
      " '43' '44' 'buzz' '46' '47' '48' '49' 'buzz' '51' '52' '53' '54' 'buzz'\n",
      " '56' '57' '58' '59' '60' '61' '62' '63' '64' 'buzz' 'fizz' '67' '68' '69'\n",
      " 'fizz' '71' '72' '73' '74' '75' '76' '77' 'fizz' '79' '80' '81' '82' '83'\n",
      " '84' 'buzz' '86' '87' '88' '89' '90' '91' '92' '93' '94' '95' '96' '97'\n",
      " '98' '99' '100']\n",
      "epoch 1100 training loss 0.661\n",
      "epoch 1200 training loss 0.592\n",
      "epoch 1300 training loss 0.522\n",
      "epoch 1400 training loss 0.438\n",
      "epoch 1500 training loss 0.376\n",
      "epoch 1600 training loss 0.321\n",
      "epoch 1700 training loss 0.279\n",
      "epoch 1800 training loss 0.246\n",
      "epoch 1900 training loss 0.218\n",
      "epoch 2000 training loss 0.196\n",
      "['1' '2' 'fizz' '4' 'buzz' 'fizz' '7' '8' 'fizz' 'buzz' '11' '12' '13'\n",
      " '14' 'fizzbuzz' '16' '17' 'fizz' 'buzz' 'buzz' '21' '22' '23' 'fizz' '25'\n",
      " '26' 'fizz' '28' '29' 'fizz' '31' '32' 'buzz' '34' 'buzz' '36' '37' '38'\n",
      " 'fizz' 'buzz' '41' 'buzz' '43' '44' 'fizzbuzz' '46' '47' 'fizz' 'buzz'\n",
      " 'buzz' 'buzz' '52' '53' 'fizz' 'buzz' '56' '57' '58' '59' '60' '61' '62'\n",
      " 'fizz' '64' 'buzz' 'fizz' '67' '68' '69' '70' '71' 'fizz' '73' '74'\n",
      " 'fizzbuzz' '76' '77' 'fizz' '79' '80' '81' '82' '83' '84' 'buzz' '86'\n",
      " '87' '88' '89' 'fizzbuzz' '91' '92' '93' '94' '95' '96' '97' '98' 'fizz'\n",
      " '100']\n",
      "epoch 2100 training loss 0.177\n",
      "epoch 2200 training loss 0.16\n",
      "epoch 2300 training loss 0.146\n",
      "epoch 2400 training loss 0.134\n",
      "epoch 2500 training loss 0.124\n",
      "epoch 2600 training loss 0.114\n",
      "epoch 2700 training loss 0.106\n",
      "epoch 2800 training loss 0.098\n",
      "epoch 2900 training loss 0.091\n",
      "epoch 3000 training loss 0.085\n",
      "['1' '2' 'fizz' '4' 'buzz' 'fizz' '7' '8' 'fizz' 'buzz' '11' '12' '13'\n",
      " '14' 'fizzbuzz' '16' '17' 'fizz' 'buzz' 'buzz' '21' '22' '23' 'fizz' '25'\n",
      " '26' 'fizz' '28' '29' 'fizz' '31' '32' 'fizz' '34' 'buzz' '36' '37' '38'\n",
      " 'fizz' 'buzz' '41' 'buzz' '43' '44' 'fizzbuzz' '46' '47' 'fizz' 'buzz'\n",
      " 'buzz' 'buzz' '52' '53' 'fizz' 'buzz' '56' '57' '58' '59' '60' '61' '62'\n",
      " 'fizz' '64' 'buzz' 'fizz' '67' '68' '69' '70' '71' 'fizz' '73' '74'\n",
      " 'fizzbuzz' '76' '77' 'fizz' '79' 'buzz' '81' '82' '83' '84' 'buzz' '86'\n",
      " '87' '88' '89' 'fizzbuzz' '91' '92' '93' '94' '95' '96' '97' '98' 'fizz'\n",
      " 'buzz']\n",
      "epoch 3100 training loss 0.079\n",
      "epoch 3200 training loss 0.074\n",
      "epoch 3300 training loss 0.07\n",
      "epoch 3400 training loss 0.065\n",
      "epoch 3500 training loss 0.061\n",
      "epoch 3600 training loss 0.058\n",
      "epoch 3700 training loss 0.055\n",
      "epoch 3800 training loss 0.052\n",
      "epoch 3900 training loss 0.049\n",
      "epoch 4000 training loss 0.046\n",
      "['1' '2' 'fizz' '4' 'buzz' 'fizz' '7' '8' 'fizz' 'buzz' '11' '12' '13'\n",
      " '14' 'fizzbuzz' '16' '17' 'fizz' 'buzz' 'buzz' '21' '22' '23' 'fizz' '25'\n",
      " '26' 'fizz' '28' '29' 'fizz' '31' '32' 'fizz' '34' 'buzz' '36' '37' '38'\n",
      " 'fizz' 'buzz' '41' 'buzz' '43' '44' 'fizzbuzz' '46' '47' 'fizz' 'buzz'\n",
      " 'buzz' 'fizz' '52' '53' 'fizz' 'buzz' '56' '57' '58' '59' '60' '61' '62'\n",
      " 'fizz' '64' 'buzz' 'fizz' '67' '68' '69' 'fizz' '71' 'fizz' '73' '74'\n",
      " 'fizzbuzz' '76' '77' 'fizz' '79' 'buzz' '81' '82' '83' '84' 'buzz' '86'\n",
      " '87' '88' '89' 'fizzbuzz' '91' '92' '93' '94' '95' '96' '97' '98' 'fizz'\n",
      " 'buzz']\n",
      "epoch 4100 training loss 0.044\n",
      "epoch 4200 training loss 0.041\n",
      "epoch 4300 training loss 0.039\n",
      "epoch 4400 training loss 0.037\n",
      "epoch 4500 training loss 0.036\n",
      "epoch 4600 training loss 0.034\n",
      "epoch 4700 training loss 0.033\n",
      "epoch 4800 training loss 0.031\n",
      "epoch 4900 training loss 0.03\n",
      "epoch 5000 training loss 0.029\n",
      "['1' '2' 'fizz' '4' 'buzz' 'fizz' '7' '8' 'fizz' 'buzz' '11' '12' '13'\n",
      " '14' 'fizzbuzz' '16' '17' 'fizz' 'buzz' 'buzz' '21' '22' '23' 'fizz' '25'\n",
      " '26' 'fizz' '28' '29' 'fizz' '31' '32' 'fizz' '34' 'buzz' '36' '37'\n",
      " 'buzz' 'fizz' 'buzz' '41' 'fizz' '43' '44' 'fizzbuzz' '46' '47' 'fizz'\n",
      " 'buzz' 'buzz' 'fizz' '52' '53' 'fizz' 'buzz' '56' 'fizz' '58' '59'\n",
      " 'fizzbuzz' '61' '62' 'fizz' '64' 'buzz' 'fizz' '67' '68' '69' 'fizz' '71'\n",
      " 'fizz' '73' '74' 'fizzbuzz' '76' '77' 'fizz' '79' 'buzz' '81' '82' '83'\n",
      " '84' 'buzz' '86' '87' '88' '89' 'fizzbuzz' '91' '92' '93' '94' 'buzz'\n",
      " '96' '97' '98' 'fizz' 'buzz']\n",
      "epoch 5100 training loss 0.027\n",
      "epoch 5200 training loss 0.026\n",
      "epoch 5300 training loss 0.025\n",
      "epoch 5400 training loss 0.024\n",
      "epoch 5500 training loss 0.023\n",
      "epoch 5600 training loss 0.023\n",
      "epoch 5700 training loss 0.022\n",
      "epoch 5800 training loss 0.021\n",
      "epoch 5900 training loss 0.02\n",
      "epoch 6000 training loss 0.02\n",
      "['1' '2' 'fizz' '4' 'buzz' 'fizz' '7' '8' 'fizz' 'buzz' '11' '12' '13'\n",
      " '14' 'fizzbuzz' '16' '17' 'fizz' 'buzz' 'buzz' '21' '22' '23' 'fizz' '25'\n",
      " '26' 'fizz' '28' '29' 'fizz' '31' '32' 'fizz' '34' 'buzz' '36' '37'\n",
      " 'buzz' 'fizz' 'buzz' '41' 'fizz' '43' '44' 'fizzbuzz' '46' '47' 'fizz'\n",
      " 'buzz' 'buzz' 'fizz' '52' '53' 'fizz' 'buzz' '56' 'fizz' '58' '59'\n",
      " 'fizzbuzz' '61' '62' 'fizz' '64' 'buzz' 'fizz' '67' '68' '69' 'fizz' '71'\n",
      " 'fizz' '73' '74' 'fizzbuzz' '76' '77' 'fizz' '79' 'buzz' '81' '82' '83'\n",
      " '84' 'buzz' '86' '87' '88' '89' 'fizzbuzz' '91' '92' '93' '94' 'buzz'\n",
      " '96' '97' '98' 'fizz' 'buzz']\n",
      "epoch 6100 training loss 0.019\n",
      "epoch 6200 training loss 0.018\n",
      "epoch 6300 training loss 0.018\n",
      "epoch 6400 training loss 0.017\n",
      "epoch 6500 training loss 0.017\n",
      "epoch 6600 training loss 0.016\n",
      "epoch 6700 training loss 0.016\n",
      "epoch 6800 training loss 0.016\n",
      "epoch 6900 training loss 0.015\n",
      "epoch 7000 training loss 0.015\n",
      "['1' '2' 'fizz' '4' 'buzz' 'fizz' '7' '8' 'fizz' 'buzz' '11' '12' '13'\n",
      " '14' 'fizzbuzz' '16' '17' 'fizz' 'buzz' 'buzz' '21' '22' '23' 'fizz' '25'\n",
      " '26' 'fizz' '28' '29' 'fizz' '31' '32' 'fizz' '34' 'buzz' '36' '37'\n",
      " 'buzz' 'fizz' 'buzz' '41' 'fizz' '43' '44' 'fizzbuzz' '46' '47' 'fizz'\n",
      " 'fizz' 'buzz' 'fizz' '52' '53' 'fizz' 'buzz' '56' 'fizz' '58' '59'\n",
      " 'fizzbuzz' '61' '62' 'fizz' '64' 'buzz' 'fizz' '67' '68' 'fizz' 'fizz'\n",
      " '71' 'fizz' '73' '74' 'fizzbuzz' '76' '77' 'fizz' '79' 'buzz' '81' '82'\n",
      " '83' '84' 'buzz' '86' '87' '88' '89' 'fizzbuzz' '91' '92' '93' '94'\n",
      " 'buzz' '96' '97' '98' 'fizz' 'buzz']\n",
      "epoch 7100 training loss 0.014\n",
      "epoch 7200 training loss 0.014\n",
      "epoch 7300 training loss 0.014\n",
      "epoch 7400 training loss 0.013\n",
      "epoch 7500 training loss 0.013\n",
      "epoch 7600 training loss 0.013\n",
      "epoch 7700 training loss 0.012\n",
      "epoch 7800 training loss 0.012\n",
      "epoch 7900 training loss 0.012\n",
      "epoch 8000 training loss 0.012\n",
      "['1' '2' 'fizz' '4' 'buzz' 'fizz' '7' '8' 'fizz' 'buzz' '11' '12' '13'\n",
      " '14' 'fizzbuzz' '16' '17' 'fizz' 'buzz' 'buzz' '21' '22' '23' 'fizz' '25'\n",
      " '26' 'fizz' '28' '29' 'fizz' '31' '32' 'fizz' '34' 'buzz' '36' '37'\n",
      " 'buzz' 'fizz' 'buzz' '41' 'fizz' '43' '44' 'fizzbuzz' '46' '47' 'fizz'\n",
      " 'fizz' 'buzz' 'fizz' '52' '53' 'fizz' 'buzz' '56' 'fizz' '58' '59'\n",
      " 'fizzbuzz' '61' '62' 'fizz' '64' 'buzz' 'fizz' '67' '68' 'fizz' 'fizz'\n",
      " '71' 'fizz' '73' '74' 'fizzbuzz' '76' '77' 'fizz' '79' 'buzz' '81' '82'\n",
      " '83' '84' 'buzz' '86' '87' '88' '89' 'fizzbuzz' '91' '92' '93' '94'\n",
      " 'buzz' '96' '97' '98' 'fizz' 'buzz']\n",
      "epoch 8100 training loss 0.011\n",
      "epoch 8200 training loss 0.011\n",
      "epoch 8300 training loss 0.011\n",
      "epoch 8400 training loss 0.011\n",
      "epoch 8500 training loss 0.01\n",
      "epoch 8600 training loss 0.01\n",
      "epoch 8700 training loss 0.01\n",
      "epoch 8800 training loss 0.01\n",
      "epoch 8900 training loss 0.01\n",
      "epoch 9000 training loss 0.009\n",
      "['1' '2' 'fizz' '4' 'buzz' 'fizz' '7' '8' 'fizz' 'buzz' '11' '12' '13'\n",
      " '14' 'fizzbuzz' '16' '17' 'fizz' 'buzz' 'buzz' '21' '22' '23' 'fizz' '25'\n",
      " '26' 'fizz' '28' '29' 'fizz' '31' '32' 'fizz' '34' 'buzz' '36' '37'\n",
      " 'buzz' 'fizz' 'buzz' '41' 'fizz' '43' '44' 'fizzbuzz' '46' '47' 'fizz'\n",
      " 'fizz' 'buzz' 'fizz' '52' '53' 'fizz' 'buzz' '56' 'fizz' '58' '59'\n",
      " 'fizzbuzz' '61' '62' 'fizz' '64' 'buzz' 'fizz' '67' '68' 'fizz' 'fizz'\n",
      " '71' 'fizz' '73' '74' 'fizzbuzz' '76' '77' 'fizz' '79' 'buzz' '81' '82'\n",
      " '83' '84' 'buzz' '86' '87' '88' '89' 'fizzbuzz' '91' '92' '93' '94'\n",
      " 'buzz' '96' '97' '98' 'fizz' 'buzz']\n",
      "epoch 9100 training loss 0.009\n",
      "epoch 9200 training loss 0.009\n",
      "epoch 9300 training loss 0.009\n",
      "epoch 9400 training loss 0.009\n",
      "epoch 9500 training loss 0.009\n",
      "epoch 9600 training loss 0.008\n",
      "epoch 9700 training loss 0.008\n",
      "epoch 9800 training loss 0.008\n",
      "epoch 9900 training loss 0.008\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "raw_data_test = np.arange(1, 101) # valeurs de test\n",
    "\n",
    "for epoch in range(10000):\n",
    "    for start in range(0, len(X_train), BATCH_SIZE):\n",
    "        end = start + BATCH_SIZE\n",
    "        batchX = X_train[start:end]\n",
    "        batchY = Y_train[start:end]\n",
    "\n",
    "        # prediction et calcul de la loss\n",
    "        y_pred = model(batchX)\n",
    "        loss = loss_fn(y_pred, batchY)\n",
    "    \n",
    "        # mettre les gradients à 0 avant la passe retour (backward)\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # rétro-propagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # calcul coût  (et affichage)\n",
    "    loss = loss_fn( model(X_train), Y_train)\n",
    "    if epoch%100 == 0:\n",
    "        print('epoch {} training loss {}'.format(epoch, round(loss.item(), 3)))\n",
    "\n",
    "    # visualisation des résultats en cours d'apprentissage\n",
    "    # (doit être fait sur l'ensemble de validation normalement)\n",
    "    if(epoch%1000==0):\n",
    "        Y_test_pred = model(X_test)\n",
    "        val, idx = torch.max(Y_test_pred,1)\n",
    "        ii=idx.data.numpy()\n",
    "        # numbers = np.arange(1, 101)\n",
    "        output = np.vectorize(fizz_buzz)(raw_data_test, ii)\n",
    "        print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Affichage des résultats__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6.1720, -0.4590, -1.1927, -6.0436],\n",
      "        [ 5.5546, -2.5170,  0.1957, -4.3396],\n",
      "        [-5.0009, 10.3592, -1.4913, -5.0316],\n",
      "        [ 9.2224, -5.6584, -1.8103, -1.9594],\n",
      "        [ 2.8647, -2.4296,  4.5885, -5.6959]], grad_fn=<SliceBackward0>)\n",
      "[0 0 1 0 2]\n",
      "============== Final result ============\n",
      "['1' '2' 'fizz' '4' 'buzz' 'fizz' '7' '8' 'fizz' 'buzz' '11' '12' '13'\n",
      " '14' 'fizzbuzz' '16' '17' 'fizz' 'buzz' 'buzz' '21' '22' '23' 'fizz' '25'\n",
      " '26' 'fizz' '28' '29' 'fizz' '31' '32' 'fizz' '34' 'buzz' '36' '37'\n",
      " 'buzz' 'fizz' 'buzz' '41' 'fizz' '43' '44' 'fizzbuzz' '46' '47' 'fizz'\n",
      " 'fizz' 'buzz' 'fizz' '52' '53' 'fizz' 'buzz' '56' 'fizz' '58' '59'\n",
      " 'fizzbuzz' '61' '62' 'fizz' '64' 'buzz' 'fizz' '67' '68' 'fizz' 'fizz'\n",
      " '71' 'fizz' '73' '74' 'fizzbuzz' '76' '77' 'fizz' '79' 'buzz' '81' '82'\n",
      " '83' '84' 'buzz' '86' '87' '88' '89' 'fizzbuzz' '91' '92' '93' '94'\n",
      " 'buzz' '96' '97' '98' 'fizz' 'buzz']\n",
      "============== Final result ============\n",
      "['1', '2', 'fizz', '4', 'buzz', 'fizz', '7', '8', 'fizz', 'buzz', '11', '12', '13', '14', 'fizzbuzz', '16', '17', 'fizz', 'buzz', 'buzz', '21', '22', '23', 'fizz', '25', '26', 'fizz', '28', '29', 'fizz', '31', '32', 'fizz', '34', 'buzz', '36', '37', 'buzz', 'fizz', 'buzz', '41', 'fizz', '43', '44', 'fizzbuzz', '46', '47', 'fizz', 'fizz', 'buzz', 'fizz', '52', '53', 'fizz', 'buzz', '56', 'fizz', '58', '59', 'fizzbuzz', '61', '62', 'fizz', '64', 'buzz', 'fizz', '67', '68', 'fizz', 'fizz', '71', 'fizz', '73', '74', 'fizzbuzz', '76', '77', 'fizz', '79', 'buzz', '81', '82', '83', '84', 'buzz', '86', '87', '88', '89', 'fizzbuzz', '91', '92', '93', '94', 'buzz', '96', '97', '98', 'fizz', 'buzz']\n"
     ]
    }
   ],
   "source": [
    "# Sortie finale (calcul lisible)\n",
    "Y_test_pred = model(X_test)\n",
    "print(Y_test_pred[:5])\n",
    "val, idx = torch.max(Y_test_pred,1)\n",
    "ii=idx.data.numpy()\n",
    "print(ii[:5])\n",
    "output = np.vectorize(fizz_buzz)(raw_data_test, ii)\n",
    "print(\"============== Final result ============\")\n",
    "print(output)\n",
    "\n",
    "# Sortie finale (calcul plus compact des predictions)\n",
    "Y_test_pred = model(X_test)\n",
    "predictions = zip(range(1, 101), list(Y_test_pred.max(1)[1].data.tolist()))\n",
    "print(\"============== Final result ============\")\n",
    "print ([fizz_buzz(i, x) for (i, x) in predictions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Calcul des performances (classification accuracy)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 2 1 0 0 1 2]\n",
      "\n",
      "\n",
      "Test acc:  0.86\n"
     ]
    }
   ],
   "source": [
    "Y_test = np.array([fizz_buzz_encode(i) for i in range(start_test, end_test)])\n",
    "print(Y_test[:10])\n",
    "print(\"\\n\\nTest acc: \", np.mean(Y_test == ii))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Avec un validation set !__\n",
    "\n",
    "On choisit par exemple de mettre de côté 100 exemples d'entrainement pour constituer un jeu de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[440 873 317 353  41]\n",
      "100 822\n"
     ]
    }
   ],
   "source": [
    "NUM_VAL = 100 \n",
    "\n",
    "# sélection aléatoire\n",
    "p = np.random.permutation(range(len(X_train)))\n",
    "print(p[:5])\n",
    "\n",
    "# train / val \n",
    "X_val , Y_val  = X_train [p[-NUM_VAL:],:] , Y_train [p[-NUM_VAL:]]\n",
    "X_train , Y_train = X_train[p[:- NUM_VAL],:] , Y_train [p[:- NUM_VAL]]\n",
    "print(len(X_val), len(X_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 training loss 1.185\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 1 0 1]\n",
      " train acc :  0.544\n",
      " val acc :  0.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100 training loss 1.116\n",
      "epoch 200 training loss 1.102\n",
      "epoch 300 training loss 1.076\n",
      "epoch 400 training loss 1.043\n",
      "epoch 500 training loss 0.988\n",
      "epoch 600 training loss 0.925\n",
      "epoch 700 training loss 0.852\n",
      "epoch 800 training loss 0.77\n",
      "epoch 900 training loss 0.682\n",
      "epoch 1000 training loss 0.594\n",
      "[0 0 0 0 0 0 0 1 0 0]\n",
      "[0 0 0 0 0 0 0 1 0 1]\n",
      " train acc :  0.787\n",
      " val acc :  0.69\n",
      "epoch 1100 training loss 0.519\n",
      "epoch 1200 training loss 0.454\n",
      "epoch 1300 training loss 0.401\n",
      "epoch 1400 training loss 0.355\n",
      "epoch 1500 training loss 0.315\n",
      "epoch 1600 training loss 0.279\n",
      "epoch 1700 training loss 0.25\n",
      "epoch 1800 training loss 0.226\n",
      "epoch 1900 training loss 0.205\n",
      "epoch 2000 training loss 0.187\n",
      "[0 0 0 0 0 0 0 1 0 0]\n",
      "[0 0 0 0 0 0 0 1 0 1]\n",
      " train acc :  0.955\n",
      " val acc :  0.79\n",
      "epoch 2100 training loss 0.17\n",
      "epoch 2200 training loss 0.155\n",
      "epoch 2300 training loss 0.143\n",
      "epoch 2400 training loss 0.132\n",
      "epoch 2500 training loss 0.123\n",
      "epoch 2600 training loss 0.114\n",
      "epoch 2700 training loss 0.106\n",
      "epoch 2800 training loss 0.099\n",
      "epoch 2900 training loss 0.093\n",
      "epoch 3000 training loss 0.087\n",
      "[0 0 0 0 0 0 0 1 0 1]\n",
      "[0 0 0 0 0 0 0 1 0 1]\n",
      " train acc :  0.987\n",
      " val acc :  0.86\n",
      "epoch 3100 training loss 0.081\n",
      "epoch 3200 training loss 0.076\n",
      "epoch 3300 training loss 0.071\n",
      "epoch 3400 training loss 0.067\n",
      "epoch 3500 training loss 0.063\n",
      "epoch 3600 training loss 0.059\n",
      "epoch 3700 training loss 0.056\n",
      "epoch 3800 training loss 0.053\n",
      "epoch 3900 training loss 0.05\n",
      "epoch 4000 training loss 0.048\n",
      "[0 0 0 0 0 0 0 1 0 1]\n",
      "[0 0 0 0 0 0 0 1 0 1]\n",
      " train acc :  0.999\n",
      " val acc :  0.88\n",
      "epoch 4100 training loss 0.045\n",
      "epoch 4200 training loss 0.043\n",
      "epoch 4300 training loss 0.041\n",
      "epoch 4400 training loss 0.039\n",
      "epoch 4500 training loss 0.037\n",
      "epoch 4600 training loss 0.035\n",
      "epoch 4700 training loss 0.034\n",
      "epoch 4800 training loss 0.032\n",
      "epoch 4900 training loss 0.031\n",
      "epoch 5000 training loss 0.03\n",
      "[0 0 0 0 0 0 0 1 0 1]\n",
      "[0 0 0 0 0 0 0 1 0 1]\n",
      " train acc :  1.0\n",
      " val acc :  0.89\n",
      "epoch 5100 training loss 0.029\n",
      "epoch 5200 training loss 0.027\n",
      "epoch 5300 training loss 0.026\n",
      "epoch 5400 training loss 0.025\n",
      "epoch 5500 training loss 0.025\n",
      "epoch 5600 training loss 0.024\n",
      "epoch 5700 training loss 0.023\n",
      "epoch 5800 training loss 0.022\n",
      "epoch 5900 training loss 0.021\n",
      "epoch 6000 training loss 0.021\n",
      "[0 0 0 0 0 0 0 1 0 1]\n",
      "[0 0 0 0 0 0 0 1 0 1]\n",
      " train acc :  1.0\n",
      " val acc :  0.9\n",
      "epoch 6100 training loss 0.02\n",
      "epoch 6200 training loss 0.02\n",
      "epoch 6300 training loss 0.019\n",
      "epoch 6400 training loss 0.018\n",
      "epoch 6500 training loss 0.018\n",
      "epoch 6600 training loss 0.017\n",
      "epoch 6700 training loss 0.017\n",
      "epoch 6800 training loss 0.016\n",
      "epoch 6900 training loss 0.016\n",
      "epoch 7000 training loss 0.016\n",
      "[0 0 0 0 0 0 0 1 0 1]\n",
      "[0 0 0 0 0 0 0 1 0 1]\n",
      " train acc :  1.0\n",
      " val acc :  0.91\n",
      "epoch 7100 training loss 0.015\n",
      "epoch 7200 training loss 0.015\n",
      "epoch 7300 training loss 0.014\n",
      "epoch 7400 training loss 0.014\n",
      "epoch 7500 training loss 0.014\n",
      "epoch 7600 training loss 0.013\n",
      "epoch 7700 training loss 0.013\n",
      "epoch 7800 training loss 0.013\n",
      "epoch 7900 training loss 0.013\n",
      "epoch 8000 training loss 0.012\n",
      "[0 0 0 0 0 0 0 1 0 1]\n",
      "[0 0 0 0 0 0 0 1 0 1]\n",
      " train acc :  1.0\n",
      " val acc :  0.91\n",
      "epoch 8100 training loss 0.012\n",
      "epoch 8200 training loss 0.012\n",
      "epoch 8300 training loss 0.011\n",
      "epoch 8400 training loss 0.011\n",
      "epoch 8500 training loss 0.011\n",
      "epoch 8600 training loss 0.011\n",
      "epoch 8700 training loss 0.011\n",
      "epoch 8800 training loss 0.01\n",
      "epoch 8900 training loss 0.01\n",
      "epoch 9000 training loss 0.01\n",
      "[0 0 0 0 0 0 0 1 0 1]\n",
      "[0 0 0 0 0 0 0 1 0 1]\n",
      " train acc :  1.0\n",
      " val acc :  0.91\n",
      "epoch 9100 training loss 0.01\n",
      "epoch 9200 training loss 0.01\n",
      "epoch 9300 training loss 0.009\n",
      "epoch 9400 training loss 0.009\n",
      "epoch 9500 training loss 0.009\n",
      "epoch 9600 training loss 0.009\n",
      "epoch 9700 training loss 0.009\n",
      "epoch 9800 training loss 0.009\n",
      "epoch 9900 training loss 0.008\n"
     ]
    }
   ],
   "source": [
    "# nouveau code d'apprentissage\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "# nombre de neurones dans la couche cachée\n",
    "NUM_HIDDEN = 100\n",
    "\n",
    "# définition du MLP à 1 couche cachée (non linearite ReLU)\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(NUM_DIGITS, NUM_HIDDEN),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(NUM_HIDDEN, 4)\n",
    "    )\n",
    "\n",
    "# fonction de coût\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "# optimiseur --> choix du learning rate \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.05)\n",
    "\n",
    "for epoch in range(10000):\n",
    "    for start in range(0, len(X_train), BATCH_SIZE):\n",
    "        end = start + BATCH_SIZE\n",
    "        batchX = X_train[start:end]\n",
    "        batchY = Y_train[start:end]\n",
    "\n",
    "        # prediction et calcul de la loss\n",
    "        y_pred = model(batchX)\n",
    "        loss = loss_fn(y_pred, batchY)\n",
    "    \n",
    "        # mettre les gradients à 0 avant la passe retour (backward)\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # rétro-propagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # calcul coût  (et affichage)\n",
    "    loss = loss_fn( model(X_train), Y_train)\n",
    "    if epoch%100 == 0:\n",
    "        print('epoch {} training loss {}'.format(epoch, round(loss.item(), 3)))\n",
    "\n",
    "    # visualisation des résultats en cours d'apprentissage\n",
    "    # cette fois-ci sur l'ensemble de validation\n",
    "    if(epoch%1000==0):\n",
    "        # train acc\n",
    "        Y_train_pred = model(X_train)\n",
    "        pred, idx = torch.max(Y_train_pred,1)\n",
    "        train_labels = idx.data.numpy()\n",
    "        print(train_labels[:10])\n",
    "        print(Y_train.data.numpy()[:10])\n",
    "        print(\" train acc : \" , round(np.mean(np.equal(Y_train.data.numpy(), train_labels)),3)) # TODO to fix\n",
    "        # val ac\n",
    "        Y_val_pred = model(X_val)\n",
    "        pred, idx = torch.max(Y_val_pred,1)\n",
    "        val_labels = idx.data.numpy()\n",
    "        print(\" val acc : \" , round(np.mean(np.equal(Y_val.data.numpy(), val_labels )),3)) # TODO to fix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performances sur le test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.8325, -7.0029,  3.1564, -1.3612],\n",
      "        [ 6.8665, -5.8519,  0.8981, -2.0299],\n",
      "        [-3.2836,  6.0388,  0.2730, -2.9217],\n",
      "        [ 4.0135, -2.7912,  5.5387, -5.3813],\n",
      "        [-1.0287, -4.6837,  8.5919, -2.7576]], grad_fn=<SliceBackward0>)\n",
      "[0 0 1 2 2]\n",
      "[0 0 1 0 2 1 0 0 1 2]\n",
      "\n",
      "\n",
      "Test acc:  0.96\n"
     ]
    }
   ],
   "source": [
    "# Sortie finale, \n",
    "Y_test_pred = model(X_test)\n",
    "print(Y_test_pred[:5])\n",
    "val, idx = torch.max(Y_test_pred,1)\n",
    "ii=idx.data.numpy()\n",
    "print(ii[:5])\n",
    "Y_test = np.array([fizz_buzz_encode(i) for i in range(start_test, end_test)])\n",
    "print(Y_test[:10])\n",
    "print(\"\\n\\nTest acc: \", np.mean(Y_test == ii))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Expérimentez avec d'autres hyperparamètres !__\n",
    "\n",
    "* learning rate\n",
    "* optimizer\n",
    "* scheduler\n",
    "* number of training samples\n",
    "* architecture of the MLP (number of hidden units)\n",
    "* number of epochs\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
