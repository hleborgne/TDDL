{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importer toutes les librairies nécessaire\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils import *\n",
    "import time\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La tâche consiste à prédire la languue d'orighine d'un nom de famille. Pour cela, nous disposons de données sous la forme d'une liste de noms pour 18 langues différentes (données téléchargeable [ici](https://download.pytorch.org/tutorial/data.zip), télécharger les données et décompresser le zip). \n",
    "Le modèle souhaité prend un nom de famille en entrée et prédit l'index de l'une des 18 classes en sortie. Un nom de famille peut être vu comme une séquence de lettres danbs un alphabet fixé. \n",
    "Questions :\n",
    "\n",
    "- Comment modéliser les entrées\n",
    "- Quel modele statistique/neuronnal choisir ?\n",
    "- Quelle prédiction peut être envisagée ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modélisation et apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'alphabet est composé de 57 caractères : 26 lettres majuscules, 26 lettres minuscules, 5 caractères spéciaux (espace, point, virgule, apostrophe, point-virgule).\n",
    "`all_letters = string.ascii_letters + \" .,;'\"\n",
    "`\n",
    "\n",
    "Les accents sont ignorées avec la fonction `unicodeToAscii` qui remplace les caractères accentués par leur équivalent non accentué. De plus, on latinise les noms arabes, chinois, japonais, coréens, ... (voir dans les données, les noms ont été latinisés)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get training data\n",
    "DATAPATH='data/names/'\n",
    "train_data,all_categories = get_language_data(DATAPATH)\n",
    "n_categories = len(all_categories)\n",
    "\n",
    "print(f\"There are {n_letters} letters \\n{all_letters}\")\n",
    "n_data = 0\n",
    "print(f'There are {n_categories} languages.\\nNumber of family name per language:')\n",
    "for categ in train_data.keys():\n",
    "    print('   {}\\t {}'.format(categ, len(train_data[categ]) ))\n",
    "    n_data += len(train_data[categ])\n",
    "print(f\"The dataset contains {n_data} family names.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data['Arabic'][:5])\n",
    "print(train_data['Chinese'][:5])\n",
    "print(train_data['French'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn_wo_optimizer(model, criterion, learning_rate, category_tensor, line_tensor):\n",
    "    hidden = model.initHidden()\n",
    "\n",
    "    model.zero_grad()\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = model(line_tensor[i], hidden)\n",
    "\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "\n",
    "    return output, loss.item()\n",
    "\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "def training_loop(model, criterion, learning_rate, train_data, all_categories, n_iters, plot_every=1000, print_every=5000):\n",
    "    current_loss = 0\n",
    "    all_losses = []\n",
    "    start = time.time()\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        category, line, category_tensor, line_tensor = randomTrainingExample(all_categories,train_data)\n",
    "        output, loss = train_rnn_wo_optimizer(model, criterion, learning_rate, category_tensor, line_tensor)\n",
    "        current_loss += loss\n",
    "\n",
    "        # Print iter number, loss, name and guess\n",
    "        if iter % print_every == 0:\n",
    "            guess, guess_i = categoryFromOutput(output,all_categories)\n",
    "            correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "            print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
    "\n",
    "        # Add current loss avg to list of losses\n",
    "        if iter % plot_every == 0:\n",
    "            all_losses.append(current_loss / plot_every)\n",
    "            current_loss = 0\n",
    "    return all_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 100000\n",
    "\n",
    "### create model\n",
    "n_hidden = 128\n",
    "model = RNN(n_letters, n_hidden, n_categories)\n",
    "\n",
    "#### training\n",
    "criterion = torch.nn.NLLLoss()\n",
    "learning_rate = 0.005 \n",
    "all_losses = training_loop(model, criterion, learning_rate, train_data, all_categories, n_iters=n_iters)\n",
    "### save model\n",
    "torch.save(model, 'char-rnn-classification.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_rnn(model, line):\n",
    "    hidden = model.initHidden()\n",
    "    for i in range(line.size()[0]):\n",
    "        output, hidden = model(line[i], hidden)\n",
    "    return output\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_lstm(model, line):\n",
    "    hidden = model.initHidden()\n",
    "    # for i in range(line.size()[0]):\n",
    "    output, hidden = model(line)\n",
    "    return output\n",
    "\n",
    "def get_confusion_matrix(model, train_data, all_categories, n_categories):\n",
    "    confusion = torch.zeros(n_categories, n_categories) \n",
    "    print(\"----------------\\n   Effectifs\\n----------------\")\n",
    "    for categ in train_data.keys():\n",
    "        print(\"   {}\\t {}\".format(categ, len(train_data[categ])))\n",
    "        for name in train_data[categ]:\n",
    "            \n",
    "            if isinstance(model, RNN):\n",
    "                output = evaluate_rnn(model, lineToTensor(name))\n",
    "            else:\n",
    "                output = evaluate_lstm(model, lineToTensor(name))\n",
    "            guess, guess_i = categoryFromOutput(output, all_categories) # get the index of the max log-probability\n",
    "            category_i = all_categories.index(categ)\n",
    "            confusion[category_i][guess_i] += 1\n",
    "\n",
    "    effectif = confusion.sum(dim=0) # sum of each column\n",
    "    print(\"----------------\\n   Scores\\n----------------\")\n",
    "    for i in range(n_categories):\n",
    "        confusion[i] = confusion[i] / (1e-16 + confusion[i].sum())\n",
    "        print(\"   {} \\t {:2.1%}\".format(all_categories[i], (confusion[i][i]).item()))\n",
    "    print(\"------\")\n",
    "    print(\"Global (flat) \\t {:2.1%}\".format(confusion.diag().mean().item()))\n",
    "    weighted_conf = confusion.diag() * (effectif / effectif.sum())\n",
    "    print(\"Global (wght) \\t {:2.1%}\".format(weighted_conf.sum().item()))\n",
    "    print(\"-----------\\n\")\n",
    "    return confusion\n",
    "\n",
    "def plot_confusion_matrix(confusion, all_categories):\n",
    "    sns.heatmap(confusion, fmt=\"g\", xticklabels=all_categories, yticklabels=all_categories)\n",
    "    plt.show()\n",
    "\n",
    "def plot_all_losses(all_losses):\n",
    "    plt.figure()\n",
    "    plt.plot(all_losses)\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('training loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_losses(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluer les performances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn = torch.load('char-rnn-classification.pt')\n",
    "confusion_matrix = get_confusion_matrix(model_rnn, train_data, all_categories, n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(confusion_matrix, all_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On utilise la matrice de confusion pour évaluer les performances du modèle. La matrice de confusion est une matrice de taille 18x18 où chaque ligne correspond à la classe réelle et chaque colonne à la classe prédite. La diagonale de la matrice correspond aux prédictions correctes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring avec tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant log les informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, criterion, learning_rate, train_data, all_categories, n_iters, tb_writer, plot_every=1000, print_every=5000):\n",
    "    current_loss = 0\n",
    "    all_losses = []\n",
    "    start = time.time()\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        category, line, category_tensor, line_tensor = randomTrainingExample(all_categories,train_data)\n",
    "        output, loss = train_rnn_wo_optimizer(model,   criterion, learning_rate, category_tensor, line_tensor)\n",
    "        current_loss += loss\n",
    "\n",
    "        # Print iter number, loss, name and guess\n",
    "        if iter % print_every == 0:\n",
    "            guess, guess_i = categoryFromOutput(output,all_categories)\n",
    "            correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "            print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            tb_writer.add_scalar('Training loss', current_loss / plot_every, iter)\n",
    "            all_losses.append(current_loss / plot_every)\n",
    "            current_loss = 0\n",
    "    tb_writer.flush()\n",
    "    tb_writer.close()\n",
    "    return all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_writer = SummaryWriter('log/nlp_scratch_exp_1')\n",
    "\n",
    "n_hidden = 128\n",
    "model = RNN(n_letters, n_hidden, n_categories)\n",
    "\n",
    "### Tensorboard visualization of the network\n",
    "# - create (any valid) input data\n",
    "# - visualize the built model in tensorboeard\n",
    "category, line, category_tensor, line_tensor = randomTrainingExample(all_categories,train_data)\n",
    "hidden = model.initHidden()\n",
    "tb_writer.add_graph(model, (line_tensor[0], hidden  ))\n",
    "\n",
    "#### training\n",
    "criterion = torch.nn.NLLLoss() # the RNN already has a softmax as output\n",
    "learning_rate = 0.005\n",
    "\n",
    "training_loop(model, criterion, learning_rate, train_data, all_categories, n_iters = 100000, tb_writer=tb_writer)\n",
    "torch.save(model, 'char-rnn-classification.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualiser le réseau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lancer tensorboard avec `tensorboard --logdir=6_nlp_scratch/log`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('char-rnn-classification.pt')\n",
    "category, line, category_tensor, line_tensor = randomTrainingExample(all_categories, train_data)\n",
    "hidden = model.initHidden()\n",
    "tb_writer.add_graph(model, (line_tensor[0], hidden))\n",
    "tb_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implémentation LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self,in_size,hidden_size, nb_layer, nb_classes):\n",
    "        super(LSTMNet,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.nb_layer = nb_layer\n",
    "        self.nb_classes = nb_classes\n",
    "        # change 'bidirectional' to get a BiLSTM\n",
    "        # batch_first=False --> input and output tensors are provided as (seq, batch, feature)\n",
    "        self.lstm = nn.LSTM(in_size,hidden_size,nb_layer,batch_first=False,bidirectional=False)\n",
    "        self.fc = nn.Linear(hidden_size,nb_classes)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # initial states; x.size(1) = batch_size avec batch_first=False\n",
    "        h0 = torch.zeros(self.nb_layer, x.size(1), self.hidden_size)#.to(device)\n",
    "        c0 = torch.zeros(self.nb_layer, x.size(1), self.hidden_size)#.to(device)\n",
    "        out,(hn,cn) = self.lstm(x, (h0,c0)) # self.lstm(x) : zero par défaut \n",
    "        out = self.fc(out[-1,:,:]) # dernière couche cachée de la séquence avec batch_first=False\n",
    "        # out = self.fc(out[:,-1,:]) # idem avec batch_first=True\n",
    "        out = self.softmax(out)\n",
    "        return out,hn\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def training_loop(model, optimizer, n_iters, tb_writer = None, plot_every = 1000, print_every= 5000):\n",
    "    start = time.time()\n",
    "    all_losses = []\n",
    "    current_loss = 0\n",
    "    print(f\"Training with a model of {n_params(model)} params \")\n",
    "    \n",
    "    for iter in range(1, n_iters + 1):\n",
    "        category, line, category_tensor, line_tensor = randomTrainingExample(all_categories,train_data)\n",
    "        if isinstance(model, RNN):\n",
    "            hidden = model.initHidden()\n",
    "        model.zero_grad()\n",
    "        if isinstance(model, RNN):\n",
    "            for i in range(line_tensor.size()[0]):\n",
    "                output, hidden = model(line_tensor[i], hidden)\n",
    "        else:\n",
    "            output, _ = model(line_tensor)\n",
    "        loss = criterion(output,category_tensor)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        current_loss += loss.item()\n",
    "        if iter % print_every == 0:\n",
    "            guess, guess_i = categoryFromOutput(output,all_categories)\n",
    "            correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "            print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            if tb_writer is not None:\n",
    "                tb_writer.add_scalar('Training loss', current_loss / plot_every, iter)\n",
    "            all_losses.append(current_loss / plot_every)\n",
    "            current_loss = 0\n",
    "\n",
    "    if tb_writer is not None:\n",
    "        tb_writer.flush()\n",
    "        tb_writer.close()\n",
    "    return all_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_writer = SummaryWriter('log/lstm')\n",
    "n_hidden = 64\n",
    "num_layers = 1\n",
    "model = LSTMNet(n_letters, n_hidden, num_layers, n_categories)#.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "n_iters = 100000\n",
    "training_loop(model, optimizer, n_iters, tb_writer)\n",
    "### save model\n",
    "torch.save(model, 'char-lstm-classification.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = torch.load('char-lstm-classification.pt')\n",
    "tb_writer = SummaryWriter('log/lstm')\n",
    "# voir le réseau\n",
    "category, line, category_tensor, line_tensor = randomTrainingExample(all_categories, train_data)\n",
    "hidden = model.initHidden()\n",
    "tb_writer.add_graph(model, (line_tensor))\n",
    "tb_writer.close()\n",
    "confusion = get_confusion_matrix(model, train_data, all_categories, n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(confusion, all_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaison RNN / LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_writer = SummaryWriter('log/rnn_adam')\n",
    "n_hidden = 128\n",
    "model = RNN(n_letters, n_hidden, n_categories)\n",
    "#### training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "current_loss = 0\n",
    "n_iters = 100000\n",
    "\n",
    "all_losses = training_loop(model, optimizer, n_iters, tb_writer)\n",
    "torch.save(model, 'char-rnn-adam-classification.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_losses(all_losses)\n",
    "confusion = get_confusion_matrix(model, train_data, all_categories, n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(confusion, all_categories)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
