{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# TD 04 GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# torch stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Génération des données\n",
    "\n",
    "- un cercle de rayon 3 centré en (0,0)\n",
    "- une sinusoide d'amplitude 1 et de fréquence 6 $\\pi$\n",
    "- une bande délimitée par deux  sinusoïdes (répartition graduelle en tanh)\n",
    "\n",
    "\n",
    "Les données seront légerement perturbées par un bruit gaussien d'amplitude 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return N data drawn according to the wanted density\n",
    "def f_data(N, model='circle'):\n",
    "  eps = np.random.randn(N) # Gaussian noise\n",
    "  if model == 'circle':\n",
    "    t = np.random.rand(N) # Uniform -> values in [0,1]\n",
    "    return np.column_stack(\n",
    "      (3 * np.cos(2 * np.pi * t) + 0.1 * eps, 3 * np.sin(2 * np.pi * t) + 0.1 * eps))\n",
    "\n",
    "  z1 = 3*np.random.randn(N) # Gaussian\n",
    "  if model == 'simple_sin':\n",
    "    return np.column_stack((z1 + 0.1 * eps,np.cos(z1) + 0.1 * eps))\n",
    "  elif model == 'double_sin':\n",
    "    z2 = 3*np.random.randn(N) # Gaussian (2)\n",
    "    return np.column_stack((z1+0.1*eps,np.cos(z1)+np.tanh(z2)+0.1*eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"circle\"\n",
    "if model == \"circle\":\n",
    "    t=np.arange(0,1.1,0.025)\n",
    "    plt.plot(3*np.cos(t*2*np.pi),3*np.sin(t*2*np.pi), 'r-')\n",
    "if model == \"simple_sin\":\n",
    "    xx = np.arange(-3,3,0.25)\n",
    "    plt.plot(3*xx,np.cos(3*xx), 'r-')\n",
    "if model == \"double_sin\":\n",
    "    xx = np.arange(-3,3,0.25)\n",
    "    plt.plot(3*xx,np.cos(3*xx)+1, 'r-')\n",
    "    plt.plot(3*xx,np.cos(3*xx)-1, 'r-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design des réseaux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Générateur qui doit chercher à tromper le discriminateur\n",
    "\n",
    "Première couche est l'espace du latent `sz_latent`, une couche cachée de taille `sz_hidden` et une couche de sortie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "  def __init__(self, sz_latent,sz_hidden):\n",
    "    super(Generator, self).__init__()\n",
    "    self.fc1 = nn.Linear(sz_latent,sz_hidden)\n",
    "    self.fc2 = nn.Linear(sz_hidden,sz_hidden)\n",
    "    self.fout = nn.Linear(sz_hidden,2)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = self.fout(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le réseau critique, est un MLP qui doit déterminer si les données sont réelles ou fausses.\n",
    "\n",
    "Il comporte trois couches, la première étant de taille `sz`. La taille des deux couches suivantes est deux fois moindre que sa précédente. La décision finale est la probabilité que la donnée d’entrée soit réelle (ou fake)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "  def __init__(self, sz):\n",
    "    super(Discriminator, self).__init__()\n",
    "    self.fc1 = nn.Linear(2,sz)\n",
    "    self.fc2 = nn.Linear(sz,int(sz/2))\n",
    "    self.fc3 = nn.Linear(int(sz/2),int(sz/4))\n",
    "    self.fout = nn.Linear(int(sz/4),1)\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = F.relu(self.fc3(x))\n",
    "    x = F.sigmoid(self.fout(x)) # decision (proba)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(v):\n",
    "    return v.data.storage().tolist()\n",
    "\n",
    "def train(batch_size, model, device, G, D, criterion, d_optimizer, g_optimizer, latent_dim, epochs):\n",
    "  for epoch in range(epochs):\n",
    "      for ii in range(20):  # train D for 20 steps\n",
    "        D.zero_grad() # could be d_optimizer.zero_grad() since the optimizer is specific to the model\n",
    "\n",
    "        # train D on real data\n",
    "        d_real_data = (torch.FloatTensor(f_data(batch_size,model))).to(device)\n",
    "        d_real_decision = D(d_real_data)\n",
    "        d_real_error = criterion(d_real_decision, torch.ones([batch_size,1]).to(device))\n",
    "        d_real_error.backward() # compute/store gradients, but don't change params\n",
    "\n",
    "        # train D on fake data\n",
    "        d_gen_seed = (torch.FloatTensor(torch.randn(batch_size,latent_dim ) )).to(device)\n",
    "        d_fake_data = G( d_gen_seed ).detach()  # detach to avoid training G on these labels\n",
    "        d_fake_decision = D(d_fake_data)\n",
    "        d_fake_error = criterion(d_fake_decision, torch.zeros([batch_size,1]).to(device))\n",
    "        d_fake_error.backward()\n",
    "        d_optimizer.step()     # Only optimizes D's parameters; changes based on stored gradients from backward()\n",
    "\n",
    "        dre, dfe = extract(d_real_error)[0], extract(d_fake_error)[0]\n",
    "\n",
    "      for ii in range(20):  # train G for 20 steps\n",
    "        G.zero_grad()\n",
    "\n",
    "        g_gen_seed = (torch.FloatTensor( torch.randn(batch_size,latent_dim ))).to(device)\n",
    "        g_fake_data = G( g_gen_seed )\n",
    "        dg_fake_decision = D(g_fake_data)\n",
    "        g_error = criterion(dg_fake_decision, torch.ones([batch_size,1]).to(device))  # Train G to pretend it's genuine\n",
    "\n",
    "        g_error.backward()\n",
    "        g_optimizer.step()  # Only optimizes G's parameters\n",
    "\n",
    "        ge = extract(g_error)[0]\n",
    "      if epoch % 20 ==0:\n",
    "        print(\"Epoch %s: D (%1.4f real_err, %1.4f fake_err) G (%1.4f err) \" % (epoch, dre, dfe, ge))\n",
    "\n",
    "      if epoch % 60 == 0:\n",
    "        g_gen_seed = (torch.FloatTensor( torch.randn(1000,latent_dim ))).to(device)\n",
    "        g_fake_data = G( g_gen_seed ).detach().to(\"cpu\")\n",
    "        \n",
    "        \n",
    "        # plot ground truth\n",
    "        if model == \"circle\":\n",
    "          t=np.arange(0,1.1,0.025)\n",
    "          plt.plot(3*np.cos(t*2*np.pi),3*np.sin(t*2*np.pi), 'r-')\n",
    "        if model == \"simple_sin\":\n",
    "          xx = np.arange(-3,3,0.25)\n",
    "          plt.plot(3*xx,np.cos(3*xx), 'r-')\n",
    "        if model == \"double_sin\":\n",
    "          xx = np.arange(-3,3,0.25)\n",
    "          plt.plot(3*xx,np.cos(3*xx)+1, 'r-')\n",
    "          plt.plot(3*xx,np.cos(3*xx)-1, 'r-')\n",
    "\n",
    "        plt.plot(g_fake_data[:,0],g_fake_data[:,1],'b.')\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_params_training(latent_dim = 2):\n",
    "    G = Generator(latent_dim,32).to(device)\n",
    "    D = Discriminator(32).to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    d_optimizer = optim.SGD(D.parameters(), lr=1e-3, momentum=0.8)\n",
    "    g_optimizer = optim.SGD(G.parameters(), lr=1e-3, momentum=0.8)\n",
    "    # Adam optimizer\n",
    "    #d_optimizer = optim.Adam(D.parameters(), lr=1e-3)\n",
    "    #g_optimizer = optim.Adam(G.parameters(), lr=1e-3)\n",
    "    return G, D, criterion, d_optimizer, g_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"circle\"\n",
    "# model = \"simple_sin\"\n",
    "# model = \"double_sin\"\n",
    "latent_dim = 2\n",
    "# epochs = 3000\n",
    "epochs = 2000\n",
    "batch_size = 32\n",
    "G, D, criterion, d_optimizer, g_optimizer = get_params_training(latent_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(batch_size, model, device, G, D, criterion, d_optimizer, g_optimizer, latent_dim, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Que se passe t'il si l'on réduit la dimension de l'espace latent ?**\n",
    "\n",
    "Si on force l'espace latent à une dimension 1, on observe que la sortie, bien que bidimiensionnelle, est contrainte sur une variété de dimension 1. Un cercle ou une simple sinudoïde sont aussi des variétés de dimension 1 (on peut les décrire avec un seul paramètre, l'angle). Mais pour les données contraintes entre deux sinusoïdes selon une densité tanhn, ça saute d'une zone de forte densité à l'autre.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = \"circle\"\n",
    "# model = \"simple_sin\"\n",
    "model = \"double_sin\"\n",
    "latent_dim = 1\n",
    "epochs = 3000\n",
    "# epochs = 2000\n",
    "G, D, criterion, d_optimizer, g_optimizer = get_params_training(latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(batch_size, model, device, G, D, criterion, d_optimizer, g_optimizer, latent_dim, epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
